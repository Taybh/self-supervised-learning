{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fde9c4c-561d-4b0d-9844-0de6eb9e14fa",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This project uses the [FastResNet](https://github.com/pytorch/ignite) architecture.  \n",
    "Credits to the original authors for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc4d37f-196b-4a78-9723-dd90f4ebe34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a965889-25b9-4c4a-93a6-6b093326c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GhostBatchNorm(nn.BatchNorm2d):\n",
    "    \"\"\"\n",
    "    From : https://github.com/davidcpage/cifar10-fast/blob/master/bag_of_tricks.ipynb\n",
    "\n",
    "    Batch norm seems to work best with batch size of around 32. The reasons presumably have to do \n",
    "    with noise in the batch statistics and specifically a balance between a beneficial regularising effect \n",
    "    at intermediate batch sizes and an excess of noise at small batches.\n",
    "    \n",
    "    Our batches are of size 512 and we can't afford to reduce them without taking a serious hit on training times, \n",
    "    but we can apply batch norm separately to subsets of a training batch. This technique, known as 'ghost' batch \n",
    "    norm, is usually used in a distributed setting but is just as useful when using large batches on a single node. \n",
    "    It isn't supported directly in PyTorch but we can roll our own easily enough.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_splits, eps=1e-05, momentum=0.1, weight=True, bias=True):\n",
    "        super(GhostBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum)\n",
    "        self.weight.data.fill_(1.0)\n",
    "        self.bias.data.fill_(0.0)\n",
    "        self.weight.requires_grad = weight\n",
    "        self.bias.requires_grad = bias        \n",
    "        self.num_splits = num_splits\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
    "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        if (self.training is True) and (mode is False):\n",
    "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
    "        return super(GhostBatchNorm, self).train(mode)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            return F.batch_norm(\n",
    "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
    "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
    "                True, self.momentum, self.eps).view(N, C, H, W) \n",
    "        else:\n",
    "            return F.batch_norm(\n",
    "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
    "                self.weight, self.bias, False, self.momentum, self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d7385f-5209-4cf3-b76c-2d48e4e24be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels, \n",
    "                 conv_ksize=3, conv_pad=1,\n",
    "                 gbn_num_splits=16):\n",
    "        super(IdentityResidualBlock, self).__init__()\n",
    "        self.res1 = nn.Sequential(\n",
    "            Conv2d(num_channels, num_channels, kernel_size=conv_ksize, padding=conv_pad, stride=1, bias=False),\n",
    "            GhostBatchNorm(num_channels, num_splits=gbn_num_splits, weight=False),\n",
    "            nn.CELU(alpha=0.3)         \n",
    "        )\n",
    "        self.res2 = nn.Sequential(\n",
    "            Conv2d(num_channels, num_channels, kernel_size=conv_ksize, padding=conv_pad, stride=1, bias=False),\n",
    "            GhostBatchNorm(num_channels, num_splits=gbn_num_splits, weight=False),\n",
    "            nn.CELU(alpha=0.3)    \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119d3b0e-3d26-4bd7-8560-7797ff8363a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We override conv2d to get proper padding for kernel size = 2   \n",
    "class Conv2d(nn.Conv2d):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Conv2d, self).__init__(*args, **kwargs)\n",
    "        if self.kernel_size == (2, 2):\n",
    "            self.forward = self.ksize_2_forward\n",
    "            self.ksize_2_padding = (0, self.padding[0], 0, self.padding[1])\n",
    "            self.padding = (0, 0)\n",
    "        \n",
    "    def ksize_2_forward(self, x):\n",
    "        x = F.pad(x, pad=self.ksize_2_padding)\n",
    "        return super(Conv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778af427-4ff1-4b92-be8f-80598928e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastResNet(nn.Module):\n",
    "        \n",
    "    def __init__(self, num_classes=10, \n",
    "                 fmap_factor=64, conv_ksize=3, conv_pad=1, \n",
    "                 gbn_num_splits=512 // 32,                  \n",
    "                 classif_scale=0.0625):\n",
    "        super(FastResNet, self).__init__()\n",
    "                \n",
    "        self.prep = nn.Sequential(\n",
    "            Conv2d(3, fmap_factor, kernel_size=conv_ksize, padding=conv_pad, stride=1, bias=False),\n",
    "            GhostBatchNorm(fmap_factor, num_splits=gbn_num_splits, weight=False),\n",
    "            nn.CELU(alpha=0.3)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            Conv2d(fmap_factor, fmap_factor * 2, kernel_size=conv_ksize, padding=conv_pad, stride=1, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            GhostBatchNorm(fmap_factor * 2, num_splits=gbn_num_splits, weight=False),\n",
    "            nn.CELU(alpha=0.3),\n",
    "            IdentityResidualBlock(fmap_factor * 2,\n",
    "                                  conv_ksize=conv_ksize, conv_pad=conv_pad, \n",
    "                                  gbn_num_splits=gbn_num_splits)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            Conv2d(fmap_factor * 2, fmap_factor * 4, kernel_size=conv_ksize, padding=conv_pad, stride=1, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            GhostBatchNorm(fmap_factor * 4, num_splits=gbn_num_splits, weight=False),\n",
    "            nn.CELU(alpha=0.3),            \n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            Conv2d(fmap_factor * 4, fmap_factor * 8, kernel_size=conv_ksize, padding=conv_pad, stride=1, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            GhostBatchNorm(fmap_factor * 8, num_splits=gbn_num_splits, weight=False),\n",
    "            nn.CELU(alpha=0.3),\n",
    "            IdentityResidualBlock(fmap_factor * 8, \n",
    "                                  conv_ksize=conv_ksize, conv_pad=conv_pad, \n",
    "                                  gbn_num_splits=gbn_num_splits)\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=4)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(fmap_factor * 8, num_classes)\n",
    "        )\n",
    "        self.scale = torch.tensor(0.0625, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prep(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool(x)\n",
    "        y = self.classifier(x)\n",
    "        return y * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608da6ac-f445-4379-9886-39727a3a9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastResNet(10, fmap_factor=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
