{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Pad, RandomCrop, RandomHorizontalFlip, RandomErasing\n",
    "from torchvision.transforms import RandAugment\n",
    "#from RandAugment import RandAugment\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "class AddTransform(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.data = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.data[index]\n",
    "        x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "def cifar10_unsupervised_dataloaders(num_labelled_indices):\n",
    "    print('wrong dataset')\n",
    "    print('Data Preparation')\n",
    "    train_transform = Compose([\n",
    "        Pad(4),\n",
    "        RandomCrop(32, fill=128),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        RandomErasing(scale=(0.1, 0.33)),\n",
    "    ])\n",
    "\n",
    "    unsupervised_train_transformation = Compose([\n",
    "        Pad(4),\n",
    "        RandomCrop(32, fill=128),\n",
    "        ToTensor(),\n",
    "        Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "    # RANDAUGMENT\n",
    "    unsupervised_train_transformation.transforms.insert(0, RandAugment(3, 9))\n",
    "\n",
    "    test_transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "    # Train dataset with and without labels\n",
    "    cifar10_train_ds = datasets.CIFAR10('~/workspace/data', train=True, download=True)\n",
    "\n",
    "    num_classes = len(cifar10_train_ds.classes)\n",
    "\n",
    "    print('Loading dataset {0} for training -- Num_samples: {1}, num_classes: {2}'.format(datasets.CIFAR10.__name__,len(cifar10_train_ds),10))\n",
    "\n",
    "    labelled_indices = []\n",
    "    unlabelled_indices = []\n",
    "\n",
    "    indices = np.random.permutation(len(cifar10_train_ds))\n",
    "    #print('length of indices: ',len(indices))\n",
    "    class_counters = list([0] * num_classes) #A list of counters to track how many labeled samples weâ€™ve selected per class.\n",
    "    #print('class_counters: ',class_counters)\n",
    "    max_counter = 10000 // num_classes #to limit each class to 10000 / 10 = 1000 labeled samples max\n",
    "\n",
    "    for i in indices:\n",
    "        dp = cifar10_train_ds[i]\n",
    "        #print('cifar10_train_ds[i]: ',cifar10_train_ds[i])\n",
    "        if len(cifar10_train_ds) < sum(class_counters):\n",
    "            print('lass_counters: ',class_counters)\n",
    "            unlabelled_indices.append(i)\n",
    "        else:\n",
    "            y = dp[1] # the label/class of the current sample\n",
    "            #print('y: ', y)\n",
    "            c = class_counters[y] # how many labeled samples we've already picked for this class\n",
    "            #print('c', c)\n",
    "            if c < max_counter:\n",
    "                class_counters[y] += 1\n",
    "                labelled_indices.append(i)\n",
    "            else:\n",
    "                unlabelled_indices.append(i)\n",
    "    \n",
    "\n",
    "    labelled_indices = random.sample(labelled_indices, num_labelled_indices) #to reduce the labeled set even further. still balanced because of how it was constructed\n",
    "    \n",
    "    # Labelled and unlabelled dataset\n",
    "    train_labelled_ds = Subset(cifar10_train_ds, labelled_indices)\n",
    "    #train_labelled_ds_t = AddTransform(train_labelled_ds, train_transform)\n",
    "    print('train_labelled_ds labels:',train_labelled_ds[0][1])\n",
    "\n",
    "    # unlabelled ds and aug ds\n",
    "    train_unlabelled_ds = Subset(cifar10_train_ds, unlabelled_indices)\n",
    "    \n",
    "    train_unlabelled_ds = ConcatDataset([train_unlabelled_ds,train_labelled_ds])\n",
    "\n",
    "    # apply transformation for both\n",
    "    #train_unlabelled_ds_t = AddTransform(train_unlabelled_ds, train_transform)\n",
    "\n",
    "    train_unlabelled_aug_ds_t = AddTransform(train_unlabelled_ds, unsupervised_train_transformation)\n",
    "\n",
    "\n",
    "    print('Labelled dataset -- Num_samples: {0}, classes: {1}, \\n Unsupervised dataset -- Num_samples {2}, Augmentation -- Num_samples: {3}'\n",
    "          .format(len(train_labelled_ds), 10, len(train_unlabelled_ds), len(train_unlabelled_aug_ds_t)))\n",
    "    '''\n",
    "    # Data loader for labeled and unlabeled train dataset\n",
    "    train_labelled = DataLoader(\n",
    "        train_labelled_ds_t,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    train_unlabelled = DataLoader(\n",
    "        train_unlabelled_ds_t,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    train_unlabelled_aug = DataLoader(\n",
    "       train_unlabelled_aug_ds_t,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Data loader for test dataset\n",
    "    cifar10_test_ds = datasets.CIFAR10('~/workspace/data', transform=test_transform, train=False, download=True)\n",
    "\n",
    "    print('Test set -- Num_samples: {0}'.format(len(cifar10_test_ds)))\n",
    "    \n",
    "  \n",
    "\n",
    "    test = DataLoader(\n",
    "        cifar10_test_ds, batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print('train_labelled: ',len(train_labelled_ds))\n",
    "    print('train_unlabelled: ',len(train_unlabelled_ds))\n",
    "    print('train_unlabelled_aug: ',len(train_unlabelled_aug_ds_t))\n",
    "    print('test total: ',len(cifar10_test_ds))\n",
    "    \n",
    "    print('test_ds type:', type(cifar10_test_ds))\n",
    "    print('test_ds type:', type(cifar10_test_ds[0]))\n",
    "    print('test_ds type:', type(cifar10_test_ds[0][0]))\n",
    "    \n",
    "    return train_labelled_ds, train_unlabelled_ds, train_unlabelled_aug, test\n",
    "    #return train_labelled, train_unlabelled, train_unlabelled_aug, test \n",
    "\n",
    "def cifar10_supervised_dataloaders(limit = 0):\n",
    "\n",
    "    if(limit > 0):\n",
    "        picks = np.random.permutation(limit)\n",
    "\n",
    "    train_ds = datasets.CIFAR10(root='./data', train=True,\n",
    "                     transform=Compose([\n",
    "                         RandomHorizontalFlip(),\n",
    "                         RandomCrop(32, 4),\n",
    "                         ToTensor(),\n",
    "                         Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                   std=[0.229, 0.224, 0.225]),\n",
    "                     ]), download=True)\n",
    "\n",
    "    if(limit > 0):\n",
    "        train_ds = Subset(train_ds, picks)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print('Loading dataset {0} for training -- Num_samples: {1}, num_classes: {2}'.format(datasets.CIFAR10.__name__,len(train_loader.dataset),10))\n",
    "\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=False, transform=Compose([\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ]), download=True),\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print('Loading dataset {0} for validating -- Num_samples: {1}, num_classes: {2}'.format(datasets.CIFAR10.__name__,len(val_loader.dataset), 10))\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
